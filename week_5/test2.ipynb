{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c16789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bafef34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/06 15:56:05 WARN Utils: Your hostname, Zambo-ROG resolves to a loopback address: 127.0.1.1; using 172.30.104.214 instead (on interface eth0)\n",
      "23/03/06 15:56:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/wtfzambo/miniconda3/envs/pyspark3/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/03/06 15:56:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('test') \\\n",
    "    .config('spark.driver.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e48548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.30.104.214:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff0da06f310>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804efdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f11cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv.gz') \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18bfbb40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!gunzip -c fhvhv_tripdata_2021-01.csv.gz | head -n 1001 > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2645c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('head.csv', parse_dates=[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81a4c301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num               object\n",
       "dispatching_base_num            object\n",
       "pickup_datetime         datetime64[ns]\n",
       "dropoff_datetime        datetime64[ns]\n",
       "PULocationID                     int64\n",
       "DOLocationID                     int64\n",
       "SR_Flag                        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "762652ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,TimestampType,true),StructField(dropoff_datetime,TimestampType,true),StructField(PULocationID,LongType,true),StructField(DOLocationID,LongType,true),StructField(SR_Flag,DoubleType,true)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9255af1",
   "metadata": {},
   "source": [
    "Integer - 4 bytes<br>\n",
    "Long - 8 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c99a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField('hvfhs_license_num', T.StringType(), True),\n",
    "    T.StructField('dispatching_base_num', T.StringType(), True),\n",
    "    T.StructField('pickup_datetime', T.TimestampType(), True),\n",
    "    T.StructField('dropoff_datetime', T.TimestampType(), True),\n",
    "    T.StructField('PULocationID', T.IntegerType(), True),\n",
    "    T.StructField('DOLocationID', T.IntegerType(), True),\n",
    "    T.StructField('SR_Flag', T.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d58d7f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .schema(schema) \\\n",
    "    .option('header', 'true') \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv') \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32cf0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "187fff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/06 16:04:37 WARN MemoryManager: Total allocation exceeds 95.00% (2,040,109,440 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 16 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('fhvhv/2021/01') \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78712e8b",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "777b4b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1335c37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2021-01-01 00:20:57|2021-01-01 00:41:24|          69|          41|   null|\n",
      "|           HV0005|              B02510|2021-01-01 02:17:52|2021-01-01 02:37:16|         243|         116|   null|\n",
      "|           HV0003|              B02878|2021-01-01 21:18:35|2021-01-01 21:32:32|          18|         259|   null|\n",
      "|           HV0003|              B02872|2021-01-01 23:25:49|2021-01-01 23:36:12|          94|          32|   null|\n",
      "|           HV0003|              B02764|2021-01-02 19:51:39|2021-01-02 20:07:16|         236|         143|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff57839c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-01 21:18:35|2021-01-01 21:32:32|          18|         259|\n",
      "|2021-01-01 23:25:49|2021-01-01 23:36:12|          94|          32|\n",
      "|2021-01-02 19:51:39|2021-01-02 20:07:16|         236|         143|\n",
      "|2021-01-02 00:02:37|2021-01-02 00:09:32|         216|         216|\n",
      "|2021-01-01 16:34:07|2021-01-01 17:02:33|         181|          75|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "    .filter(df.hvfhs_license_num == 'HV0003') \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92676a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-01|  2021-01-01|          69|          41|\n",
      "| 2021-01-01|  2021-01-01|         243|         116|\n",
      "| 2021-01-01|  2021-01-01|          18|         259|\n",
      "| 2021-01-01|  2021-01-01|          94|          32|\n",
      "| 2021-01-02|  2021-01-02|         236|         143|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4eecc5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f403115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "518e4e10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  e/9ce| 2021-01-01|  2021-01-01|          69|          41|\n",
      "|  e/9ce| 2021-01-01|  2021-01-01|         243|         116|\n",
      "|  e/b3e| 2021-01-01|  2021-01-01|          18|         259|\n",
      "|  e/b38| 2021-01-01|  2021-01-01|          94|          32|\n",
      "|  e/acc| 2021-01-02|  2021-01-02|         236|         143|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark3]",
   "language": "python",
   "name": "conda-env-pyspark3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
